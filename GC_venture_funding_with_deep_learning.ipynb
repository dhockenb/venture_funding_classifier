{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "GC_venture_funding_with_deep_learning.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhockenb/venture_funding_classifier/blob/main/GC_venture_funding_with_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XiL4BeyURsD"
      },
      "source": [
        "# Import required modules and dependencies\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler,OneHotEncoder"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GsFRRuGU9r-"
      },
      "source": [
        "# Upload applicants data.csv to Google Colab\n",
        "from google.colab import files\n",
        "\n",
        "csv_file = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSTFDSzYURsE"
      },
      "source": [
        "# Read the applicants_data.csv file into a Pandas DataFrame\n",
        "applicant_data_df = pd.read_csv(\"applicants_data.csv\")\n",
        "\n",
        "# Review the DataFrame\n",
        "applicant_data_df.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-WB0yc4URsE"
      },
      "source": [
        "# Review the data types associated with the columns\n",
        "applicant_data_df.dtypes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj7yoYaAURsE"
      },
      "source": [
        "# Drop the 'EIN' and 'NAME' columns from the DataFrame\n",
        "applicant_data_df = applicant_data_df.drop([\"EIN\", \"NAME\"], axis=1)\n",
        "\n",
        "# Review the DataFrame\n",
        "applicant_data_df.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHv_okM7URsF"
      },
      "source": [
        "# Create a list of categorical variables \n",
        "categorical_variables = [\"APPLICATION_TYPE\", \"AFFILIATION\", \"CLASSIFICATION\", \"USE_CASE\", \"ORGANIZATION\", \"INCOME_AMT\", \"SPECIAL_CONSIDERATIONS\"]\n",
        "\n",
        "# Display the categorical variables list\n",
        "categorical_variables\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IizC83_xURsF"
      },
      "source": [
        "# Create a OneHotEncoder instance\n",
        "enc = OneHotEncoder(sparse=False)\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLUlCo48URsF"
      },
      "source": [
        "# Encode the categorical variables using OneHotEncoder\n",
        "encoded_data = enc.fit_transform(applicant_data_df[categorical_variables])\n"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bj-Hc08qURsF"
      },
      "source": [
        "# Create a DataFrame with the encoded variables\n",
        "encoded_df = pd.DataFrame(encoded_data,\n",
        "                          columns=enc.get_feature_names(categorical_variables))\n",
        "\n",
        "# Review the DataFrame\n",
        "encoded_df.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r43_PQpFURsG"
      },
      "source": [
        "# Add the numerical variables from the original DataFrame to the OneHotEncoder DataFrame\n",
        "applicant_num_data_df = applicant_data_df.drop([\"APPLICATION_TYPE\", \"AFFILIATION\", \"CLASSIFICATION\", \"USE_CASE\", \"ORGANIZATION\", \"INCOME_AMT\", \"SPECIAL_CONSIDERATIONS\"], axis = 1)\n",
        "encoded_df2 = pd.concat([applicant_num_data_df, encoded_df], axis=1)\n",
        "\n",
        "# Review the Dataframe\n",
        "encoded_df2.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpmWKy35URsG"
      },
      "source": [
        "# Define the target set y as the IS_SUCCESSFUL column\n",
        "y = encoded_df2[['IS_SUCCESSFUL']]\n",
        "\n",
        "# Display a sample of y\n",
        "y.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzVEfRlaURsG"
      },
      "source": [
        "# Define features set X by selecting all columns but IS_SUCCESSFUL\n",
        "X = encoded_df2.drop(['IS_SUCCESSFUL'], axis=1)\n",
        "\n",
        "# Review the features DataFrame\n",
        "X.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_vGfjV4URsG"
      },
      "source": [
        "# Split the preprocessed data into a training and testing dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWOH8vl-URsG"
      },
      "source": [
        "# Create a StandardScaler instance\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the features training dataset\n",
        "X_scaler = scaler.fit(X_train)\n",
        "\n",
        "# Fit the scaler to the features training dataset\n",
        "X_train_scaled = X_scaler.transform(X_train)\n",
        "X_test_scaled = X_scaler.transform(X_test)\n"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binary classification model using two-layer deep neural network and relu activation function "
      ],
      "metadata": {
        "id": "34rLl-5UmzQ8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1Vgwz04URsH"
      },
      "source": [
        "# Define the the number of inputs (features) to the model\n",
        "number_input_features = len(X_train.iloc[0])\n",
        "\n",
        "# Review the number of features\n",
        "number_input_features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mcgkl8SURsH"
      },
      "source": [
        "# Define the number of neurons in the output layer\n",
        "number_output_neurons = 1"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0Q3iY3wURsH"
      },
      "source": [
        "# Define the number of hidden nodes for the first hidden layer\n",
        "hidden_nodes_layer1 = (number_input_features + 1) // 2\n",
        "\n",
        "# Review the number hidden nodes in the first layer\n",
        "hidden_nodes_layer1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux_lAsrvURsH"
      },
      "source": [
        "# Define the number of hidden nodes for the second hidden layer\n",
        "hidden_nodes_layer2 = (hidden_nodes_layer1 + 1) // 2\n",
        "\n",
        "# Review the number hidden nodes in the second layer\n",
        "hidden_nodes_layer2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kq9QWhLURsH"
      },
      "source": [
        "# Create the Sequential model instance\n",
        "nn = Sequential()\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcVrWCD_URsI"
      },
      "source": [
        "# Add the first hidden layer\n",
        "nn.add(Dense(units=hidden_nodes_layer1, activation=\"relu\", input_dim=number_input_features))\n"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ugxr6lisURsI"
      },
      "source": [
        "# Add the second hidden layer\n",
        "nn.add(Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxcGEyNoURsI"
      },
      "source": [
        "# Add the output layer to the model specifying the number of output neurons and activation function\n",
        "nn.add(Dense(units=1, activation=\"sigmoid\"))\n"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L2ZVcXgURsI"
      },
      "source": [
        "# Display the Sequential model summary\n",
        "nn.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR8r_cL1URsI"
      },
      "source": [
        "# Compile the Sequential model using binary_crossentropy loss function, adam optimizer and accuracy evaluation metric.\n",
        "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_z-FPHJURsI"
      },
      "source": [
        "# Fit the model using 50 epochs and the training data\n",
        "fit_model = nn.fit(X_train_scaled, y_train, epochs=50)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWaHGdGrURsI"
      },
      "source": [
        "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
        "model_loss, model_accuracy = nn.evaluate(X_test_scaled, y_test, verbose=2)\n",
        "\n",
        "# Display the model loss and accuracy results\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOuA28RQURsJ"
      },
      "source": [
        "# Set the model's file path\n",
        "file_path = \"AlphabetSoup.h5\"\n",
        "\n",
        "# Export model to a HDF5 file\n",
        "nn.save(file_path)\n"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slhZ-iS1URsJ"
      },
      "source": [
        "## Alternative Model 1 with three hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqbrMOgCURsJ"
      },
      "source": [
        "# Define the the number of inputs (features) to the model\n",
        "number_input_features_A1 = len(X_train.iloc[0])\n",
        "\n",
        "# Review the number of features\n",
        "number_input_features_A1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3rrh76sURsJ"
      },
      "source": [
        "# Define the number of neurons in the output layer\n",
        "number_output_neurons_A1 = 1"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMOrWGBKURsJ"
      },
      "source": [
        "# Define the number of hidden nodes for the first hidden layer\n",
        "hidden_nodes_layer1_A1 = (number_input_features + 1) // 2\n",
        "\n",
        "# Review the number of hidden nodes in the first layer\n",
        "hidden_nodes_layer1_A1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_nodes_layer2_A1 = (hidden_nodes_layer1_A1 + 1) // 2\n",
        "\n",
        "hidden_nodes_layer2_A1"
      ],
      "metadata": {
        "id": "4htEJm7zuXdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_nodes_layer3_A1 = (hidden_nodes_layer2_A1 + 1) // 2\n",
        "\n",
        "hidden_nodes_layer3_A1"
      ],
      "metadata": {
        "id": "ZTgf0oOfuqlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efNsx7TfURsK"
      },
      "source": [
        "# Create the Sequential model instance\n",
        "nn_A1 = Sequential()"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uMhdXQWURsK"
      },
      "source": [
        "# First hidden layer\n",
        "nn_A1.add(Dense(units=hidden_nodes_layer1_A1, activation=\"relu\", input_dim=number_input_features_A1))\n",
        "\n",
        "nn_A1.add(Dense(units=hidden_nodes_layer2_A1, activation=\"relu\"))\n",
        "\n",
        "nn_A1.add(Dense(units=hidden_nodes_layer3_A1, activation=\"relu\"))\n",
        "\n",
        "\n",
        "# Output layer\n",
        "nn_A1.add(Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "\n",
        "# Check the structure of the model\n",
        "nn_A1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gDpRhG7URsK"
      },
      "source": [
        "# Compile the Sequential model\n",
        "nn_A1.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE4UD4NyURsK"
      },
      "source": [
        "# Fit the model using 50 epochs and the training data\n",
        "fit_model_A1 = nn_A1.fit(X_train_scaled, y_train, epochs=50)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Alternative Model 1 Results\")\n",
        "\n",
        "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
        "model_loss, model_accuracy = nn_A1.evaluate(X_test_scaled, y_test, verbose=2)\n",
        "\n",
        "\n",
        "# Display the model loss and accuracy results\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ],
      "metadata": {
        "id": "fvNh0kASH5Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMZAmHrlURsK"
      },
      "source": [
        "## Alternative Model 2 with elu activation function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw0cxkvOURsK"
      },
      "source": [
        "# Define the the number of inputs (features) to the model\n",
        "number_input_features = len(X_train.iloc[0])\n",
        "\n",
        "# Review the number of features\n",
        "number_input_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnLSrAKJURsK"
      },
      "source": [
        "# Define the number of neurons in the output layer\n",
        "number_output_neurons_A2 = 1\n"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myK4iGVBURsK"
      },
      "source": [
        "# Define the number of hidden nodes for the first hidden layer\n",
        "hidden_nodes_layer1_A2 = (number_input_features + 1) // 2\n",
        "\n",
        "# Review the number of hidden nodes in the first layer\n",
        "hidden_nodes_layer1_A2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_nodes_layer2_A2 = (hidden_nodes_layer1_A2 + 1) // 2\n",
        "\n",
        "hidden_nodes_layer2_A2"
      ],
      "metadata": {
        "id": "ueQdChZ-xc5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB6J98OSURsL"
      },
      "source": [
        "# Create the Sequential model instance\n",
        "nn_A2 = Sequential()"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX5HLuh9URsL"
      },
      "source": [
        "# First hidden layer\n",
        "nn_A2.add(Dense(units=hidden_nodes_layer1_A2, activation=\"elu\", input_dim=number_input_features))\n",
        "\n",
        "nn_A2.add(Dense(units=hidden_nodes_layer2_A2, activation=\"elu\"))\n",
        "\n",
        "# Output layer\n",
        "nn_A2.add(Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "# Check the structure of the model\n",
        "nn_A2.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrI6zrIRURsL"
      },
      "source": [
        "# Compile the model\n",
        "nn_A2.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M3fXVn_URsL"
      },
      "source": [
        "# Fit the model\n",
        "fit_model_A2 = nn_A2.fit(X_train_scaled, y_train, epochs=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Alternative Model 2 Results\")\n",
        "\n",
        "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
        "model_loss, model_accuracy = nn_A2.evaluate(X_test_scaled, y_test, verbose=2)\n",
        "\n",
        "\n",
        "# Display the model loss and accuracy results\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ],
      "metadata": {
        "id": "FT0Lx_t2HrgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alternative Model 3 removing ASK_AMT as feature."
      ],
      "metadata": {
        "id": "6pRTHDPF6JZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = encoded_df2.drop(['ASK_AMT', 'IS_SUCCESSFUL'], axis=1)\n",
        "X.head()"
      ],
      "metadata": {
        "id": "aZ1W479m6Fst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y)"
      ],
      "metadata": {
        "id": "Vn_y0w5n61hD"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the features training dataset\n",
        "X_scaler = scaler.fit(X_train)\n",
        "\n",
        "# Fit the scaler to the features training dataset\n",
        "X_train_scaled = X_scaler.transform(X_train)\n",
        "X_test_scaled = X_scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "G-BaoY-_7AEC"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the the number of inputs (features) to the model\n",
        "number_input_features = len(X_train.iloc[0])\n",
        "\n",
        "# Review the number of features\n",
        "number_input_features"
      ],
      "metadata": {
        "id": "G23WiE8T7U2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_output_neurons_A3 = 1"
      ],
      "metadata": {
        "id": "mECgoRUk7Za5"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of hidden nodes for the first hidden layer\n",
        "hidden_nodes_layer1_A3 = (number_input_features + 1) // 2\n",
        "\n",
        "# Review the number of hidden nodes in the first layer\n",
        "hidden_nodes_layer1_A3"
      ],
      "metadata": {
        "id": "AKDpgRg27q9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_nodes_layer2_A3 = (hidden_nodes_layer1_A3 + 1) // 2\n",
        "\n",
        "hidden_nodes_layer2_A3"
      ],
      "metadata": {
        "id": "TbOZ1fuw7zxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_A3 = Sequential()"
      ],
      "metadata": {
        "id": "RIYHYeET8Ca4"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_A3.add(Dense(units=hidden_nodes_layer1_A3, activation=\"relu\", input_dim=number_input_features))\n",
        "\n",
        "nn_A3.add(Dense(units=hidden_nodes_layer2_A3, activation=\"relu\"))\n",
        "\n",
        "# Output layer\n",
        "nn_A3.add(Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "# Check the structure of the model\n",
        "nn_A3.summary()"
      ],
      "metadata": {
        "id": "YjuT6xP18LZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_A3.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "KXRmmh2t8fKG"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fit_model_A3 = nn_A3.fit(X_train_scaled, y_train, epochs=50)"
      ],
      "metadata": {
        "id": "LDGzVDNq8jTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Alternative Model 3 Results\")\n",
        "\n",
        "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
        "model_loss, model_accuracy = nn_A3.evaluate(X_test_scaled, y_test, verbose=2)\n",
        "\n",
        "\n",
        "# Display the model loss and accuracy results\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ],
      "metadata": {
        "id": "b3gLEFFy9Jf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1tXiNBZURsM"
      },
      "source": [
        "# Set the file path for the first alternative model\n",
        "file_path = \"AlphabetSoup_A1.h5\"\n",
        "\n",
        "# Export model to a HDF5 file\n",
        "nn_A1.save(file_path)\n"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9KTZW35URsM"
      },
      "source": [
        "# Set the file path for the second alternative model\n",
        "file_path = \"AlphabetSoup_A2.h5\"\n",
        "\n",
        "# Export model to a HDF5 file\n",
        "nn_A2.save(file_path)\n"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiQ2lyQuURsM"
      },
      "source": [
        "# Set the file path for the third alternative model\n",
        "file_path = \"AlphabetSoup_A3.h5\"\n",
        "\n",
        "# Export model to a HDF5 file\n",
        "nn_A3.save(file_path)"
      ],
      "execution_count": 132,
      "outputs": []
    }
  ]
}